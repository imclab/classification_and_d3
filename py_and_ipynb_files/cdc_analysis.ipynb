{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from postgresql_conn import engine\n",
    "# from pandasql import sqldf\n",
    "# import operator\n",
    "# from operator import itemgetter\n",
    "# from IPython.display import display\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "# from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "# from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_curve, roc_auc_score\n",
    "# from sklearn.feature_selection import RFE\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# %matplotlib inline\n",
    "\n",
    "# titlefont = {'fontname':'Times New Roman', 'fontweight':'bold', 'fontsize':18}\n",
    "# labelfont = {'fontname':'Times New Roman', 'fontsize':16}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####<font color='navy'>Functions</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get KNearestClassifier with maximum accuracy\n",
    "def max_knn(X_train, y_train, X_test, y_test, n=20):\n",
    "    \"\"\"\n",
    "    where n = the number of nearest neighbors (iterations)\n",
    "        X_train = X/features training set\n",
    "        y_train = y/response training set\n",
    "        X_test = X/features test set\n",
    "        y_test = y/response test set\n",
    "    \n",
    "    returns knum (int) = nearest neighbor index beginning 1 to n (e.g. 11)\n",
    "            kval (float) = nearest neighbor accuracy value (e.g. 0.6745)\n",
    "    \"\"\"\n",
    "    \n",
    "    k_list = []\n",
    "    for i in range(1, n+1):\n",
    "        temp = []\n",
    "        model = KNeighborsClassifier(n_neighbors=i).fit(X_train, y_train)\n",
    "        temp.append(\"KNN %s:\" % (i))\n",
    "        temp.append(\"%.4f\" % (accuracy_score(y_test, model.predict(X_test))))\n",
    "        k_list.append(temp)\n",
    "    k_dict = dict(k_list)\n",
    "    \n",
    "    knn = int(max(k_dict.iteritems(), key=itemgetter(1))[0].split()[1].split(':')[0]) #returns knn number\n",
    "    kvalue = float(max(k_dict.iteritems(), key=itemgetter(1))[1])                 #returns knn value\n",
    "    \n",
    "    return knn, kvalue\n",
    "\n",
    "#calculate metrics (accuracy, precision, recall, fscore)\n",
    "def metrics(X_train, y_train, X_test, y_test, classifiers=[KNeighborsClassifier(n_neighbors=20), LogisticRegression(random_state=0), SVC(probability=True), GaussianNB(), DecisionTreeClassifier(), RandomForestClassifier(), ExtraTreesClassifier(n_estimators=25, max_depth=9, random_state=0)]):\n",
    "    \"\"\"\n",
    "    where classifiers = list of classifier(s)\n",
    "        X_train = X/features training set\n",
    "        y_train = y/response training set\n",
    "        X_test = X/features test set\n",
    "        y_test = y/response test set\n",
    "        \n",
    "    returns list of lists (classifier, accuracy, precision, recall, f_score, auc)            \n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = []\n",
    "    for i in range(len(classifiers)):\n",
    "        temp = []\n",
    "        models[i] = classifiers[i].fit(X_train, y_train)\n",
    "        temp.append(titles[i])\n",
    "        temp.append(\"%.4f\" % (accuracy_score(y_test, models[i].predict(X_test))))\n",
    "        temp.append(\"%.4f\" % (precision_recall_fscore_support(y_test, models[i].predict(X_test))[0][1]))\n",
    "        temp.append(\"%.4f\" % (precision_recall_fscore_support(y_test, models[i].predict(X_test))[1][1]))\n",
    "        temp.append(\"%.4f\" % (precision_recall_fscore_support(y_test, models[i].predict(X_test))[2][1]))\n",
    "        temp.append(\"%.4f\" % (roc_auc_score(y_test, [x[1] for x in models[i].predict_proba(X_test)])))\n",
    "        metrics.append(temp)\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "def baseline(y_true):\n",
    "    \"\"\"\n",
    "    baseline function which predicts true everytime\n",
    "    \"\"\"\n",
    "    return [1 for y in range(len(y_true))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#brute force method of determining best n_estimator for ExtraTreesClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "trees = []\n",
    "\n",
    "for i in range(10, 26): #can be changed\n",
    "    temp = []\n",
    "    ex = ExtraTreesClassifier(n_estimators=i, random_state=1).fit(X_train, y_train)\n",
    "    temp.append('Extra Tree %s' % str(i))\n",
    "    temp.append(\"%.4f\" % (accuracy_score(y_test, ex.predict(X_test))))\n",
    "    temp.append(\"%.4f\" % (precision_recall_fscore_support(y_test, ex.predict(X_test))[0][1]))\n",
    "    temp.append(\"%.4f\" % (precision_recall_fscore_support(y_test, ex.predict(X_test))[1][1]))\n",
    "    temp.append(\"%.4f\" % (precision_recall_fscore_support(y_test, ex.predict(X_test))[2][1]))\n",
    "    temp.append(\"%.4f\" % (roc_auc_score(y_test, [x[1] for x in ex.predict_proba(X_test)])))\n",
    "    trees.append(temp)\n",
    "\n",
    "pd.DataFrame(trees, columns=['Classifier', 'Accuracy', 'Precision', 'Recall', 'F_Score', 'AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#brute force method of determining max_depth for ExtraTree Classifier\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "trees = []\n",
    "\n",
    "for i in range(21, 22): #can be changed\n",
    "    for j in range(12, 13): #can be changed\n",
    "        temp = []\n",
    "        ex = ExtraTreesClassifier(n_estimators=i, max_depth=j, random_state=0).fit(X_train, y_train)\n",
    "        temp.append('Extra Tree %s' % str(i))\n",
    "        temp.append('Depth %s' % str(j))\n",
    "        temp.append(\"%.4f\" % (accuracy_score(y_test, ex.predict(X_test))))\n",
    "        temp.append(\"%.4f\" % (precision_recall_fscore_support(y_test, ex.predict(X_test))[0][1]))\n",
    "        temp.append(\"%.4f\" % (precision_recall_fscore_support(y_test, ex.predict(X_test))[1][1]))\n",
    "        temp.append(\"%.4f\" % (precision_recall_fscore_support(y_test, ex.predict(X_test))[2][1]))\n",
    "        temp.append(\"%.4f\" % (roc_auc_score(y_test, [x[1] for x in ex.predict_proba(X_test)])))\n",
    "        trees.append(temp)\n",
    "ex_features = sorted(zip(X.columns, ex.feature_importances_), key=operator.itemgetter(1), reverse=True)\n",
    "#sorted(zipped, key=operator.itemgetter(1))\n",
    "a = pd.DataFrame(trees, columns=['n_estimator', 'max_depth', 'Accuracy', 'Precision', 'Recall', 'F_Score', 'AUC'])\n",
    "a   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####<font color='navy'>Case 1: Raw Hypertension Data (raw_hypertension_fullset)</font>\n",
    "####<font color='navy'>Case 2: Binary Hypertension Data (r_converted_data)</font>\n",
    "####<font color='navy'>Case 3: Raw Hypertension Data with recursive feature elimination (raw_hypertension_rfe_less)</font>\n",
    "####<font color='navy'>Case 4: Binary Hypertension Data with recursive feature elimination (r_converted_data_less)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "bp = pd.read_sql('raw_hypertension_fullset', engine)\n",
    "print bp.shape\n",
    "bp = bp.dropna()\n",
    "print bp.shape\n",
    "\n",
    "#Create training and test sets\n",
    "y = bp['highbloodstatus']\n",
    "X = bp.drop('highbloodstatus', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=0)\n",
    "\n",
    "#Check baseline model\n",
    "baseline_li = []\n",
    "base_temp = []  \n",
    "base_temp.append('Baseline')\n",
    "base_temp.append(\"%.4f\" % (accuracy_score(y_test, baseline(y_test))))\n",
    "base_temp.append(\"%.4f\" % (precision_recall_fscore_support(y_test, baseline(y_test))[0][1]))\n",
    "base_temp.append(\"%.4f\" % (precision_recall_fscore_support(y_test, baseline(y_test))[1][1]))\n",
    "base_temp.append(\"%.4f\" % (precision_recall_fscore_support(y_test, baseline(y_test))[2][1]))\n",
    "base_temp.append(\"%.4f\" % (roc_auc_score(y_test, [1 for x in range(len(y_test))])))\n",
    "baseline_li.append(base_temp)\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_li, columns=['Classifier', 'Accuracy', 'Precision', 'Recall', 'F_Score', 'AUC'])\n",
    "display(baseline_df)\n",
    "\n",
    "baseline_fpr, baseline_tpr, _ = roc_curve(y_test, [1 for x in range(len(y_test))])\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(baseline_fpr, baseline_tpr)\n",
    "plt.xlabel('False Positive Rate', **labelfont)\n",
    "plt.ylabel('True Positive Rate', **labelfont)\n",
    "plt.title('Baseline ROC Curve', **titlefont)\n",
    "\n",
    "#Build models\n",
    "knum, kval = max_knn(X_train, y_train, X_test, y_test, 18)\n",
    "\n",
    "classifiers = [KNeighborsClassifier(knum), LogisticRegression(random_state=0), SVC(probability=True), GaussianNB(), BernoulliNB(), MultinomialNB(), DecisionTreeClassifier(random_state=0), RandomForestClassifier(random_state=0), ExtraTreesClassifier(n_estimators=21, max_depth=12, random_state=0)]\n",
    "models = ['knn', 'log', 'svc', 'gaussian', 'bernoulli', 'multinomial', 'tree', 'forest', 'extrees']\n",
    "titles = ['KNN', 'Log', 'SVC', 'Gaussian', 'Bernoulli', 'Multinomial', 'Decision Tree', 'Random Forest', 'Extra Trees']\n",
    "\n",
    "#Calculate metrics\n",
    "scores = metrics(X_train, y_train, X_test, y_test, classifiers)\n",
    "metrics_df = pd.DataFrame(metrics(X_train, y_train, X_test, y_test, classifiers), columns=['Classifier', 'Accuracy', 'Precision', 'Recall', 'F_Score', 'AUC'])\n",
    "display(metrics_df)\n",
    "\n",
    "fpr_list, tpr_list = [], []\n",
    "for i in range(len(classifiers)):\n",
    "    class_ = classifiers[i].fit(X_train, y_train)\n",
    "    fpr, tpr, _ = roc_curve(y_test, [x[1] for x in class_.predict_proba(X_test)])\n",
    "    fpr_list.append(fpr)\n",
    "    tpr_list.append(tpr)\n",
    "\n",
    "#Create plots\n",
    "colors = ['Red', 'Yellow', 'Blue', 'Purple', 'Teal', 'Cyan', 'Olive', 'Green', 'Maroon']\n",
    "\n",
    "plt.figure(figsize=(15, 10)) #single plot\n",
    "for i in range(len(fpr_list)):\n",
    "    plt.plot(fpr_list[i], tpr_list[i], color=colors[i], label=titles[i])\n",
    "    plt.xlabel('False Positive Rate', **labelfont)\n",
    "    plt.ylabel('True Positive Rate', **labelfont)\n",
    "    plt.title('ROC Curves', **titlefont)\n",
    "    leg = plt.legend(loc=4, frameon=True, fontsize=12)\n",
    "    leg.get_frame().set_edgecolor('dimgrey')\n",
    "    \n",
    "plt.figure(figsize=(12,15)) #one plot for each model\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    for i in range(len(fpr_list)):\n",
    "        plt.subplot(5,2,i+1)\n",
    "        plt.plot(fpr_list[i], tpr_list[i], color=colors[i], label=\"AUC: %.4f\" % float(scores[i][5]))\n",
    "        plt.xlabel('False Positive Rate', **labelfont)\n",
    "        if i%2==0:\n",
    "            plt.ylabel('True Positive Rate', **labelfont)\n",
    "        plt.title(titles[i], **titlefont)\n",
    "        leg = plt.legend(loc=2, frameon=True, fontsize=12)\n",
    "        leg.get_frame().set_edgecolor('dimgrey')\n",
    "    plt.tight_layout(pad=0.4, w_pad=0.2, h_pad=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create baseline + classifiers model for dataset 1\n",
    "merge = \"\"\"\n",
    "SELECT Classifier, 'Model 1' AS Model, 'Raw Full (43)' AS Dataset, Accuracy, Precision, Recall, F_Score, AUC\n",
    "FROM baseline_df \n",
    "UNION \n",
    "SELECT Classifier, 'Model 1', 'Raw Full (43)', Accuracy, Precision, Recall, F_Score, AUC\n",
    "FROM metrics_df\n",
    "\"\"\"\n",
    "\n",
    "acc_rec_df_1 = sqldf(merge, globals())\n",
    "acc_rec_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create baseline + classifiers model for dataset 2\n",
    "merge = \"\"\"\n",
    "SELECT Classifier, 'Model 2' AS Model, 'Converted Full (42)' AS Dataset, Accuracy, Precision, Recall, F_Score, AUC\n",
    "FROM baseline_df \n",
    "UNION \n",
    "SELECT Classifier, 'Model 2', 'Converted Full (42)', Accuracy, Precision, Recall, F_Score, AUC\n",
    "FROM metrics_df\n",
    "\"\"\"\n",
    "\n",
    "acc_rec_df_2 = sqldf(merge, globals())\n",
    "acc_rec_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create baseline + classifiers model for dataset 3\n",
    "merge = \"\"\"\n",
    "SELECT Classifier, 'Model 3' AS Model, 'Raw Less (21)' AS Dataset, Accuracy, Precision, Recall, F_Score, AUC\n",
    "FROM baseline_df \n",
    "UNION \n",
    "SELECT Classifier, 'Model 3', 'Raw Less (21)', Accuracy, Precision, Recall, F_Score, AUC\n",
    "FROM metrics_df\n",
    "\"\"\"\n",
    "\n",
    "acc_rec_df_3 = sqldf(merge, globals())\n",
    "acc_rec_df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create baseline + classifiers model for dataset 4\n",
    "merge = \"\"\"\n",
    "SELECT Classifier, 'Model 4' AS Model, 'Converted Less (20)' AS Dataset, Accuracy, Precision, Recall, F_Score, AUC\n",
    "FROM baseline_df \n",
    "UNION \n",
    "SELECT Classifier, 'Model 4', 'Converted Less (20)', Accuracy, Precision, Recall, F_Score, AUC\n",
    "FROM metrics_df\n",
    "\"\"\"\n",
    "\n",
    "acc_rec_df_4 = sqldf(merge, globals())\n",
    "acc_rec_df_4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "union = \"\"\"\n",
    "SELECT Classifier, Model, Dataset, Accuracy, Precision, Recall, F_Score, AUC\n",
    "FROM acc_rec_df_1\n",
    "UNION\n",
    "SELECT Classifier, Model, Dataset, Accuracy, Precision, Recall, F_Score, AUC\n",
    "FROM acc_rec_df_2\n",
    "UNION\n",
    "SELECT Classifier, Model, Dataset, Accuracy, Precision, Recall, F_Score, AUC\n",
    "FROM acc_rec_df_3\n",
    "UNION\n",
    "SELECT Classifier, Model, Dataset, Accuracy, Precision, Recall, F_Score, AUC\n",
    "FROM acc_rec_df_4\n",
    "ORDER BY Model, Classifier\n",
    "\"\"\"\n",
    "\n",
    "s = sqldf(union, globals())\n",
    "s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create csv file for d3\n",
    "bar = \"\"\"\n",
    "SELECT Classifier\n",
    "    , SUM(CASE WHEN Model = 'Model 1' THEN Recall END) AS Model_1_Raw\n",
    "    , SUM(CASE WHEN Model = 'Model 2' THEN Recall END) AS Model_2_Binary\n",
    "    , SUM(CASE WHEN Model = 'Model 3' THEN Recall END) AS Model_3_Raw_RFE\n",
    "    , SUM(CASE WHEN Model = 'Model 4' THEN Recall END) AS Model_4_Binary_RFE\n",
    "FROM s\n",
    "GROUP BY Classifier\n",
    "\"\"\"\n",
    "\n",
    "sqldf(bar, globals()).to_csv('pivot_recall.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####<font color='navy'>Cross validation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#[KNeighborsClassifier(knum), LogisticRegression(), SVC(probability=True), GaussianNB(), BernoulliNB(), MultinomialNB(), DecisionTreeClassifier(), RandomForestClassifier()]\n",
    "models = [BernoulliNB(), ExtraTreesClassifier(n_estimators=19, max_depth=8, random_state=0), LogisticRegression(random_state=0), GaussianNB()]\n",
    "name = ['Binomial', 'Extra Trees', 'Log', 'Gaussian']\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "crossval = []\n",
    "\n",
    "for i in range(len(models)):\n",
    "    for j in range(len(scoring)):\n",
    "        tempscore = []    \n",
    "        tempscore.append(name[i])\n",
    "        tempscore.extend(cross_val_score(models[i], X.values, y.values, scoring=scoring[j], cv=5)) \n",
    "        crossval.append(tempscore)\n",
    "crossval_df = pd.DataFrame(crossval, columns=['Classifier', 'Accuracy', 'Precision', 'Recall', 'F_Score', 'AUC'])\n",
    "\n",
    "cv_mean = \"\"\"\n",
    "            SELECT Classifier\n",
    "                , CAST(AVG(Accuracy) AS DECIMAL(10,4)) AS Accuracy\n",
    "                , CAST(AVG(Precision) AS DECIMAL(10,4)) AS Precision\n",
    "                , CAST(AVG(Recall) AS DECIMAL(10,4)) AS Recall\n",
    "                , CAST(AVG(F_Score) AS DECIMAL(10,4)) AS F_Score\n",
    "                , CAST(AVG(AUC) AS DECIMAL(10,4)) AS AUC\n",
    "            FROM crossval_df\n",
    "            GROUP BY Classifier\n",
    "        \"\"\"\n",
    "\n",
    "sqldf(cv_mean, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check for the important features (feature importance)\n",
    "rfe_class = [LogisticRegression(random_state=0), BernoulliNB(), MultinomialNB()]\n",
    "important = []\n",
    "for i in range(len(rfe_class)):\n",
    "    temp = []\n",
    "    models[i] = RFE(rfe_class[i])\n",
    "    models[i] = models[i].fit(X_train, y_train)\n",
    "    important.append(zip(X_train.columns, models[i].support_))\n",
    "    #print models[i].n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = pd.DataFrame(important[0], columns=['Feature', 'Desired'])\n",
    "b = pd.DataFrame(important[1], columns=['Feature', 'Desired'])\n",
    "m = pd.DataFrame(important[2], columns=['Feature', 'Desired'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lbm = \"\"\"\n",
    "SELECT Feature, Log, Bernoulli, Multinomial\n",
    ", Log+Bernoulli+Multinomial AS Desired\n",
    "FROM(\n",
    "SELECT l.Feature\n",
    ", CASE WHEN l.Desired THEN 1 ELSE 0 END AS Log\n",
    ", CASE WHEN b.Desired THEN 1 ELSE 0 END AS Bernoulli\n",
    ", CASE WHEN m.Desired THEN 1 ELSE 0 END AS Multinomial\n",
    "FROM l JOIN\n",
    "b ON l.Feature = b.Feature JOIN\n",
    "m ON l.Feature = m.Feature\n",
    ") AS A\n",
    "\"\"\"\n",
    "\n",
    "#rfe_conv = sqldf(lbm, globals())#.to_csv('RFE_Converted.csv')\n",
    "#rfe_raw = sqldf(lbm, globals())\n",
    "sqldf(lbm, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heat = \"\"\"\n",
    "SELECT CASE WHEN Raw_Desired IS NULL THEN 0 ELSE Raw_Desired END AS Raw_Desired\n",
    "    , CASE WHEN Conv_Desired IS NULL THEN 0 ELSE Conv_Desired END AS Conv_Desired\n",
    "FROM(\n",
    "    SELECT rfe_conv.Feature AS Feature, rfe_raw.Desired AS Raw_Desired, rfe_conv.Desired AS Conv_Desired\n",
    "    FROM rfe_conv LEFT JOIN\n",
    "        rfe_raw ON rfe_conv.Feature = rfe_raw.Feature\n",
    "    UNION \n",
    "    SELECT rfe_raw.Feature, rfe_raw.Desired, rfe_conv.Desired\n",
    "    FROM rfe_raw LEFT JOIN\n",
    "        rfe_conv ON rfe_conv.Feature = rfe_raw.Feature\n",
    ") AS A\n",
    "\"\"\"\n",
    "heatmap = sqldf(heat, globals())\n",
    "type(heatmap)\n",
    "\n",
    "# heat = \"\"\"\n",
    "# SELECT DISTINCT Feature\n",
    "# FROM(\n",
    "# SELECT rfe_raw.Feature AS Feature, 'RAW' AS Model, rfe_raw.Desired AS Desired\n",
    "# FROM rfe_raw\n",
    "# UNION\n",
    "# SELECT rfe_conv.Feature, 'CONVERTED', rfe_conv.Desired\n",
    "# FROM rfe_conv\n",
    "# ) A\n",
    "# ORDER BY Feature\n",
    "# \"\"\"\n",
    "# print [x[0].encode('utf8') for x in sqldf(heat, globals()).values.tolist()]#.to_csv('heat_unpivot.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#heatmap_pivot = pd.DataFrame()\n",
    "heatmap = np.array(heatmap)\n",
    "heatmap_pivot = []\n",
    "temp = []\n",
    "temp2 = []\n",
    "for x in heatmap:\n",
    "\n",
    "    temp.append(x[0])\n",
    "    temp2.append(x[1])\n",
    "heatmap_pivot.append(temp)\n",
    "heatmap_pivot.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heatmap_1 = [x[:24] for x in heatmap_pivot]\n",
    "heatmap_2 = [x[24:] for x in heatmap_pivot]\n",
    "xlabel_1 = ylabel[:24]\n",
    "xlabel_2 = ylabel[24:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 1))\n",
    "sns.heatmap(heatmap_1, yticklabels=['Raw', 'Converted'], xticklabels=xlabel_1, linewidths=.5, cbar=False, ax=ax)\n",
    "ax.tick_params(axis='y', labelsize=18)\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "\n",
    "fig2, ax2 = plt.subplots(1, 1, figsize=(10, 1))\n",
    "sns.heatmap(heatmap_2, yticklabels=['Raw', 'Converted'], xticklabels=xlabel_2, linewidths=.5, cbar=False, ax=ax2)\n",
    "ax2.tick_params(axis='y', labelsize=18)\n",
    "ax2.tick_params(axis='x', labelsize=14)\n",
    "# fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 2))\n",
    "# sns.heatmap(heatmap_1, yticklabels=['Raw', 'Converted'], xticklabels=xlabel_1, linewidths=.5, cbar=False, ax=ax1)\n",
    "# sns.heatmap(heatmap_2, yticklabels=['Raw', 'Converted'], xticklabels=xlabel_2, linewidths=.5, cbar=False, ax=ax2)\n",
    "# ax1.tick_params(axis='x', labelsize=14)\n",
    "# ax1.tick_params(axis='y', labelsize=18)\n",
    "# ax2.tick_params(axis='x', labelsize=14)\n",
    "# ax2.tick_params(axis='y', labelsize=18)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.savefig('Important_Features_Heatmap.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv = pd.read_csv('RFE_Converted.csv')\n",
    "len(conv[conv['Desired'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw = pd.read_csv('RFE_Raw.csv')\n",
    "len(raw[raw['Desired'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv('acc_and_recall.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
